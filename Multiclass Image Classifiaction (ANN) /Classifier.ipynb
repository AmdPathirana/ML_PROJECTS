{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e6aa49-1eb2-4745-b9de-f28f6e3ad54c",
   "metadata": {},
   "source": [
    "## Installation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f6584d-f4d7-40de-908f-1c43896dd7eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import the tensorflow\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf92fe6c-a13d-4667-aa1a-a3c90614a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import other packages \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35e43e-fd52-4561-9322-3f38aa59f3e2",
   "metadata": {},
   "source": [
    "## Data Preprocessig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910cc4f9-90d0-4545-8466-97a63e6ec059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data set \n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8123662c-8fab-4735-8cd6-340e13296dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data set \n",
    "data = fashion_mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33dfddec-2e06-4eb8-82c6-d7b25f38109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (60000, 28, 28)\n",
      "y_train: (60000,)\n",
      "x_test: (10000, 28, 28)\n",
      "y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Check the shape of the data set \n",
    "print(\"x_train: {}\".format(x_train.shape))\n",
    "print(\"y_train: {}\".format(y_train.shape))\n",
    "print(\"x_test: {}\".format(x_test.shape))\n",
    "print(\"y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e8791f6-d944-49d0-a59b-9fe190bd07e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x16d6273a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check images \n",
    "plt.imshow(x_train[1])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c386fcb4-3e76-4da2-9380-ab6a43e8b2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   1,   0,   0,   0,   0,  41, 188, 103,\n",
       "         54,  48,  43,  87, 168, 133,  16,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   1,   0,   0,   0,  49, 136, 219, 216, 228, 236,\n",
       "        255, 255, 255, 255, 217, 215, 254, 231, 160,  45,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,  14, 176, 222, 224, 212, 203, 198, 196,\n",
       "        200, 215, 204, 202, 201, 201, 201, 209, 218, 224, 164,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0, 188, 219, 200, 198, 202, 198, 199, 199,\n",
       "        201, 196, 198, 198, 200, 200, 200, 200, 201, 200, 225,  41,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  51, 219, 199, 203, 203, 212, 238, 248, 250,\n",
       "        245, 249, 246, 247, 252, 248, 235, 207, 203, 203, 222, 140,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 116, 226, 206, 204, 207, 204, 101,  75,  47,\n",
       "         73,  48,  50,  45,  51,  63, 113, 222, 202, 206, 220, 224,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 200, 222, 209, 203, 215, 200,   0,  70,  98,\n",
       "          0, 103,  59,  68,  71,  49,   0, 219, 206, 214, 210, 250,  38,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 247, 218, 212, 210, 215, 214,   0, 254, 243,\n",
       "        139, 255, 174, 251, 255, 205,   0, 215, 217, 214, 208, 220,  95,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,  45, 226, 214, 214, 215, 224, 205,   0,  42,  35,\n",
       "         60,  16,  17,  12,  13,  70,   0, 189, 216, 212, 206, 212, 156,\n",
       "          0,   0],\n",
       "       [  0,   0,   0, 164, 235, 214, 211, 220, 216, 201,  52,  71,  89,\n",
       "         94,  83,  78,  70,  76,  92,  87, 206, 207, 222, 213, 219, 208,\n",
       "          0,   0],\n",
       "       [  0,   0,   0, 106, 187, 223, 237, 248, 211, 198, 252, 250, 248,\n",
       "        245, 248, 252, 253, 250, 252, 239, 201, 212, 225, 215, 193, 113,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,  17,  54, 159, 222, 193, 208, 192, 197,\n",
       "        200, 200, 200, 200, 201, 203, 195, 210, 165,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  47, 225, 192, 214, 203, 206,\n",
       "        204, 204, 205, 206, 204, 212, 197, 218, 107,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   1,   6,   0,  46, 212, 195, 212, 202, 206,\n",
       "        205, 204, 205, 206, 204, 212, 200, 218,  91,   0,   3,   1,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   1,   0,  11, 197, 199, 205, 202, 205,\n",
       "        206, 204, 205, 207, 204, 205, 205, 218,  77,   0,   5,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   3,   0,   2, 191, 198, 201, 205, 206,\n",
       "        205, 205, 206, 209, 206, 199, 209, 219,  74,   0,   5,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   2,   0,   0, 188, 197, 200, 207, 207,\n",
       "        204, 207, 207, 210, 208, 198, 207, 221,  72,   0,   4,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   2,   0,   0, 215, 198, 203, 206, 208,\n",
       "        205, 207, 207, 210, 208, 200, 202, 222,  75,   0,   4,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   1,   0,   0, 212, 198, 209, 206, 209,\n",
       "        206, 208, 207, 211, 206, 205, 198, 221,  80,   0,   3,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   1,   0,   0, 204, 201, 205, 208, 207,\n",
       "        205, 211, 205, 210, 210, 209, 195, 221,  96,   0,   3,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   1,   0,   0, 202, 201, 205, 209, 207,\n",
       "        205, 213, 206, 210, 209, 210, 194, 217, 105,   0,   2,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   1,   0,   0, 204, 204, 205, 208, 207,\n",
       "        205, 215, 207, 210, 208, 211, 193, 213, 115,   0,   2,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 204, 207, 207, 208, 206,\n",
       "        206, 215, 210, 210, 207, 212, 195, 210, 118,   0,   2,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   1,   0,   0, 198, 208, 208, 208, 204,\n",
       "        207, 212, 212, 210, 207, 211, 196, 207, 121,   0,   1,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   1,   0,   0, 198, 210, 207, 208, 206,\n",
       "        209, 213, 212, 211, 207, 210, 197, 207, 124,   0,   1,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 172, 210, 203, 201, 199,\n",
       "        204, 207, 205, 204, 201, 205, 197, 206, 127,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 188, 221, 214, 234, 236,\n",
       "        238, 244, 244, 244, 240, 243, 214, 224, 162,   0,   2,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   1,   0,   0, 139, 146, 130, 135, 135,\n",
       "        137, 125, 124, 125, 121, 119, 114, 130,  76,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check a single photo \n",
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217ff190-a327-4027-b484-2047884575de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255, 72.94035223214286)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#values the range in between 0-255 in a single photo \n",
    "np.min(x_train), np.max(x_train), np.average(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c19318-34da-4c16-9bb4-53c8f8129560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the numbr of clases \n",
    "np.min(y_train), np.max(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d1793-7e37-40b3-9aa3-69b0adf4d423",
   "metadata": {},
   "source": [
    "Consits with 10 clasess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5cd42f-e9c1-413d-ab48-5241bee79482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create class labels \n",
    "class_names = [\n",
    "    '0 T-shirt/top', '1 Trouser', '2 Pullover', '3 Dress', '4 Coat',\n",
    "    '5 Sandal', '6 Shirt', '7 Sneaker', '8 Bag', '9 Ankle boot'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a44d1388-a868-4bdd-ac99-b665beae2091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create function to get the name \n",
    "def get_name(x):\n",
    "    name=\"none\"\n",
    "    for i in range(x+1):\n",
    "        name=class_names[i]\n",
    "    return name \n",
    "        \n",
    "    \n",
    "\n",
    "#check the labels and their corrosponding names \n",
    "z=0\n",
    "plt.imshow(x_train[z])\n",
    "name=get_name(y_train[z])\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d87211-d8df-41d8-9d85-3f537e249e8c",
   "metadata": {},
   "source": [
    "## Normalization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7916a381-2179-496f-9bee-6ecc7b96549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6f99c20-aaee-43ac-9110-36157278e5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.16078431, 0.7372549 , 0.40392157, 0.21176471, 0.18823529,\n",
       "        0.16862745, 0.34117647, 0.65882353, 0.52156863, 0.0627451 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.00392157, 0.        ,\n",
       "        0.        , 0.        , 0.19215686, 0.53333333, 0.85882353,\n",
       "        0.84705882, 0.89411765, 0.9254902 , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.85098039, 0.84313725, 0.99607843,\n",
       "        0.90588235, 0.62745098, 0.17647059, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05490196, 0.69019608, 0.87058824, 0.87843137, 0.83137255,\n",
       "        0.79607843, 0.77647059, 0.76862745, 0.78431373, 0.84313725,\n",
       "        0.8       , 0.79215686, 0.78823529, 0.78823529, 0.78823529,\n",
       "        0.81960784, 0.85490196, 0.87843137, 0.64313725, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.7372549 , 0.85882353, 0.78431373, 0.77647059, 0.79215686,\n",
       "        0.77647059, 0.78039216, 0.78039216, 0.78823529, 0.76862745,\n",
       "        0.77647059, 0.77647059, 0.78431373, 0.78431373, 0.78431373,\n",
       "        0.78431373, 0.78823529, 0.78431373, 0.88235294, 0.16078431,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.2       ,\n",
       "        0.85882353, 0.78039216, 0.79607843, 0.79607843, 0.83137255,\n",
       "        0.93333333, 0.97254902, 0.98039216, 0.96078431, 0.97647059,\n",
       "        0.96470588, 0.96862745, 0.98823529, 0.97254902, 0.92156863,\n",
       "        0.81176471, 0.79607843, 0.79607843, 0.87058824, 0.54901961,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.45490196,\n",
       "        0.88627451, 0.80784314, 0.8       , 0.81176471, 0.8       ,\n",
       "        0.39607843, 0.29411765, 0.18431373, 0.28627451, 0.18823529,\n",
       "        0.19607843, 0.17647059, 0.2       , 0.24705882, 0.44313725,\n",
       "        0.87058824, 0.79215686, 0.80784314, 0.8627451 , 0.87843137,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.78431373,\n",
       "        0.87058824, 0.81960784, 0.79607843, 0.84313725, 0.78431373,\n",
       "        0.        , 0.2745098 , 0.38431373, 0.        , 0.40392157,\n",
       "        0.23137255, 0.26666667, 0.27843137, 0.19215686, 0.        ,\n",
       "        0.85882353, 0.80784314, 0.83921569, 0.82352941, 0.98039216,\n",
       "        0.14901961, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.96862745,\n",
       "        0.85490196, 0.83137255, 0.82352941, 0.84313725, 0.83921569,\n",
       "        0.        , 0.99607843, 0.95294118, 0.54509804, 1.        ,\n",
       "        0.68235294, 0.98431373, 1.        , 0.80392157, 0.        ,\n",
       "        0.84313725, 0.85098039, 0.83921569, 0.81568627, 0.8627451 ,\n",
       "        0.37254902, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.17647059, 0.88627451,\n",
       "        0.83921569, 0.83921569, 0.84313725, 0.87843137, 0.80392157,\n",
       "        0.        , 0.16470588, 0.1372549 , 0.23529412, 0.0627451 ,\n",
       "        0.06666667, 0.04705882, 0.05098039, 0.2745098 , 0.        ,\n",
       "        0.74117647, 0.84705882, 0.83137255, 0.80784314, 0.83137255,\n",
       "        0.61176471, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.64313725, 0.92156863,\n",
       "        0.83921569, 0.82745098, 0.8627451 , 0.84705882, 0.78823529,\n",
       "        0.20392157, 0.27843137, 0.34901961, 0.36862745, 0.3254902 ,\n",
       "        0.30588235, 0.2745098 , 0.29803922, 0.36078431, 0.34117647,\n",
       "        0.80784314, 0.81176471, 0.87058824, 0.83529412, 0.85882353,\n",
       "        0.81568627, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.41568627, 0.73333333,\n",
       "        0.8745098 , 0.92941176, 0.97254902, 0.82745098, 0.77647059,\n",
       "        0.98823529, 0.98039216, 0.97254902, 0.96078431, 0.97254902,\n",
       "        0.98823529, 0.99215686, 0.98039216, 0.98823529, 0.9372549 ,\n",
       "        0.78823529, 0.83137255, 0.88235294, 0.84313725, 0.75686275,\n",
       "        0.44313725, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.06666667, 0.21176471, 0.62352941, 0.87058824, 0.75686275,\n",
       "        0.81568627, 0.75294118, 0.77254902, 0.78431373, 0.78431373,\n",
       "        0.78431373, 0.78431373, 0.78823529, 0.79607843, 0.76470588,\n",
       "        0.82352941, 0.64705882, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.18431373, 0.88235294, 0.75294118,\n",
       "        0.83921569, 0.79607843, 0.80784314, 0.8       , 0.8       ,\n",
       "        0.80392157, 0.80784314, 0.8       , 0.83137255, 0.77254902,\n",
       "        0.85490196, 0.41960784, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "        0.02352941, 0.        , 0.18039216, 0.83137255, 0.76470588,\n",
       "        0.83137255, 0.79215686, 0.80784314, 0.80392157, 0.8       ,\n",
       "        0.80392157, 0.80784314, 0.8       , 0.83137255, 0.78431373,\n",
       "        0.85490196, 0.35686275, 0.        , 0.01176471, 0.00392157,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.        , 0.04313725, 0.77254902, 0.78039216,\n",
       "        0.80392157, 0.79215686, 0.80392157, 0.80784314, 0.8       ,\n",
       "        0.80392157, 0.81176471, 0.8       , 0.80392157, 0.80392157,\n",
       "        0.85490196, 0.30196078, 0.        , 0.01960784, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01176471, 0.        , 0.00784314, 0.74901961, 0.77647059,\n",
       "        0.78823529, 0.80392157, 0.80784314, 0.80392157, 0.80392157,\n",
       "        0.80784314, 0.81960784, 0.80784314, 0.78039216, 0.81960784,\n",
       "        0.85882353, 0.29019608, 0.        , 0.01960784, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00784314, 0.        , 0.        , 0.7372549 , 0.77254902,\n",
       "        0.78431373, 0.81176471, 0.81176471, 0.8       , 0.81176471,\n",
       "        0.81176471, 0.82352941, 0.81568627, 0.77647059, 0.81176471,\n",
       "        0.86666667, 0.28235294, 0.        , 0.01568627, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00784314, 0.        , 0.        , 0.84313725, 0.77647059,\n",
       "        0.79607843, 0.80784314, 0.81568627, 0.80392157, 0.81176471,\n",
       "        0.81176471, 0.82352941, 0.81568627, 0.78431373, 0.79215686,\n",
       "        0.87058824, 0.29411765, 0.        , 0.01568627, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.        , 0.        , 0.83137255, 0.77647059,\n",
       "        0.81960784, 0.80784314, 0.81960784, 0.80784314, 0.81568627,\n",
       "        0.81176471, 0.82745098, 0.80784314, 0.80392157, 0.77647059,\n",
       "        0.86666667, 0.31372549, 0.        , 0.01176471, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.        , 0.        , 0.8       , 0.78823529,\n",
       "        0.80392157, 0.81568627, 0.81176471, 0.80392157, 0.82745098,\n",
       "        0.80392157, 0.82352941, 0.82352941, 0.81960784, 0.76470588,\n",
       "        0.86666667, 0.37647059, 0.        , 0.01176471, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.        , 0.        , 0.79215686, 0.78823529,\n",
       "        0.80392157, 0.81960784, 0.81176471, 0.80392157, 0.83529412,\n",
       "        0.80784314, 0.82352941, 0.81960784, 0.82352941, 0.76078431,\n",
       "        0.85098039, 0.41176471, 0.        , 0.00784314, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.        , 0.        , 0.8       , 0.8       ,\n",
       "        0.80392157, 0.81568627, 0.81176471, 0.80392157, 0.84313725,\n",
       "        0.81176471, 0.82352941, 0.81568627, 0.82745098, 0.75686275,\n",
       "        0.83529412, 0.45098039, 0.        , 0.00784314, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.8       , 0.81176471,\n",
       "        0.81176471, 0.81568627, 0.80784314, 0.80784314, 0.84313725,\n",
       "        0.82352941, 0.82352941, 0.81176471, 0.83137255, 0.76470588,\n",
       "        0.82352941, 0.4627451 , 0.        , 0.00784314, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.        , 0.        , 0.77647059, 0.81568627,\n",
       "        0.81568627, 0.81568627, 0.8       , 0.81176471, 0.83137255,\n",
       "        0.83137255, 0.82352941, 0.81176471, 0.82745098, 0.76862745,\n",
       "        0.81176471, 0.4745098 , 0.        , 0.00392157, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.        , 0.        , 0.77647059, 0.82352941,\n",
       "        0.81176471, 0.81568627, 0.80784314, 0.81960784, 0.83529412,\n",
       "        0.83137255, 0.82745098, 0.81176471, 0.82352941, 0.77254902,\n",
       "        0.81176471, 0.48627451, 0.        , 0.00392157, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.6745098 , 0.82352941,\n",
       "        0.79607843, 0.78823529, 0.78039216, 0.8       , 0.81176471,\n",
       "        0.80392157, 0.8       , 0.78823529, 0.80392157, 0.77254902,\n",
       "        0.80784314, 0.49803922, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.7372549 , 0.86666667,\n",
       "        0.83921569, 0.91764706, 0.9254902 , 0.93333333, 0.95686275,\n",
       "        0.95686275, 0.95686275, 0.94117647, 0.95294118, 0.83921569,\n",
       "        0.87843137, 0.63529412, 0.        , 0.00784314, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.        , 0.        , 0.54509804, 0.57254902,\n",
       "        0.50980392, 0.52941176, 0.52941176, 0.5372549 , 0.49019608,\n",
       "        0.48627451, 0.49019608, 0.4745098 , 0.46666667, 0.44705882,\n",
       "        0.50980392, 0.29803922, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the normalized values of a particualr image \n",
    "x_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d561f37-56d5-49da-9cc5-5eb41a7d5abf",
   "metadata": {},
   "source": [
    "#### Since we use ANN we need to faltten the data (convert 2 dimention arrays in to 1D vector)\n",
    "\n",
    "Vector:\n",
    " \n",
    "A vector is a one-dimensional array that stores a sequence of elements.\n",
    "It is a collection of numbers (or other data types) arranged in a single row or column.\n",
    "In programming, a vector is often represented as a list or an array with a single index, containing elements arranged in a linear sequence.\n",
    "For example, [1, 2, 3, 4, 5] or (1, 2, 3, 4, 5) could represent a vector of five elements.\n",
    "\n",
    "2D Array (Matrix):\n",
    "\n",
    "A 2D array, also known as a matrix, organizes elements in rows and columns.\n",
    "It consists of multiple rows and columns, forming a grid-like structure.\n",
    "In programming, a 2D array is typically represented as a nested array or list where each row contains elements.\n",
    "For example, [[1, 2, 3], [4, 5, 6], [7, 8, 9]] represents a 3x3 matrix with three rows and three columns.\n",
    "\n",
    "\n",
    "Differences:\n",
    "\n",
    "Dimensionality: Vectors are one-dimensional structures, while 2D arrays (matrices) are two-dimensional.\n",
    "Representation: Vectors are typically represented as a single row or column of elements, while matrices are represented as a grid with rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dcac5d6-be78-4bf2-b73c-e5d653ba9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399e5b2-9741-4273-be3c-204b81054a7f",
   "metadata": {},
   "source": [
    "In NumPy, the -1 argument in the reshape() function is a placeholder that instructs NumPy to automatically infer or calculate the size of that particular dimension based on the total number of elements in the array and the known size of the other dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2803f05-94fa-417b-8984-d257208bf886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7d8f081-da6b-45c3-b063-cefc5ffb962b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.16078431, 0.7372549 , 0.40392157, 0.21176471, 0.18823529,\n",
       "       0.16862745, 0.34117647, 0.65882353, 0.52156863, 0.0627451 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.19215686, 0.53333333, 0.85882353, 0.84705882, 0.89411765,\n",
       "       0.9254902 , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.85098039, 0.84313725, 0.99607843, 0.90588235, 0.62745098,\n",
       "       0.17647059, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.05490196, 0.69019608, 0.87058824, 0.87843137,\n",
       "       0.83137255, 0.79607843, 0.77647059, 0.76862745, 0.78431373,\n",
       "       0.84313725, 0.8       , 0.79215686, 0.78823529, 0.78823529,\n",
       "       0.78823529, 0.81960784, 0.85490196, 0.87843137, 0.64313725,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.7372549 ,\n",
       "       0.85882353, 0.78431373, 0.77647059, 0.79215686, 0.77647059,\n",
       "       0.78039216, 0.78039216, 0.78823529, 0.76862745, 0.77647059,\n",
       "       0.77647059, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
       "       0.78823529, 0.78431373, 0.88235294, 0.16078431, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.2       , 0.85882353, 0.78039216, 0.79607843,\n",
       "       0.79607843, 0.83137255, 0.93333333, 0.97254902, 0.98039216,\n",
       "       0.96078431, 0.97647059, 0.96470588, 0.96862745, 0.98823529,\n",
       "       0.97254902, 0.92156863, 0.81176471, 0.79607843, 0.79607843,\n",
       "       0.87058824, 0.54901961, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.45490196,\n",
       "       0.88627451, 0.80784314, 0.8       , 0.81176471, 0.8       ,\n",
       "       0.39607843, 0.29411765, 0.18431373, 0.28627451, 0.18823529,\n",
       "       0.19607843, 0.17647059, 0.2       , 0.24705882, 0.44313725,\n",
       "       0.87058824, 0.79215686, 0.80784314, 0.8627451 , 0.87843137,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.78431373, 0.87058824, 0.81960784,\n",
       "       0.79607843, 0.84313725, 0.78431373, 0.        , 0.2745098 ,\n",
       "       0.38431373, 0.        , 0.40392157, 0.23137255, 0.26666667,\n",
       "       0.27843137, 0.19215686, 0.        , 0.85882353, 0.80784314,\n",
       "       0.83921569, 0.82352941, 0.98039216, 0.14901961, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.96862745, 0.85490196, 0.83137255, 0.82352941, 0.84313725,\n",
       "       0.83921569, 0.        , 0.99607843, 0.95294118, 0.54509804,\n",
       "       1.        , 0.68235294, 0.98431373, 1.        , 0.80392157,\n",
       "       0.        , 0.84313725, 0.85098039, 0.83921569, 0.81568627,\n",
       "       0.8627451 , 0.37254902, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.17647059, 0.88627451, 0.83921569,\n",
       "       0.83921569, 0.84313725, 0.87843137, 0.80392157, 0.        ,\n",
       "       0.16470588, 0.1372549 , 0.23529412, 0.0627451 , 0.06666667,\n",
       "       0.04705882, 0.05098039, 0.2745098 , 0.        , 0.74117647,\n",
       "       0.84705882, 0.83137255, 0.80784314, 0.83137255, 0.61176471,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.64313725, 0.92156863, 0.83921569, 0.82745098, 0.8627451 ,\n",
       "       0.84705882, 0.78823529, 0.20392157, 0.27843137, 0.34901961,\n",
       "       0.36862745, 0.3254902 , 0.30588235, 0.2745098 , 0.29803922,\n",
       "       0.36078431, 0.34117647, 0.80784314, 0.81176471, 0.87058824,\n",
       "       0.83529412, 0.85882353, 0.81568627, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.41568627, 0.73333333,\n",
       "       0.8745098 , 0.92941176, 0.97254902, 0.82745098, 0.77647059,\n",
       "       0.98823529, 0.98039216, 0.97254902, 0.96078431, 0.97254902,\n",
       "       0.98823529, 0.99215686, 0.98039216, 0.98823529, 0.9372549 ,\n",
       "       0.78823529, 0.83137255, 0.88235294, 0.84313725, 0.75686275,\n",
       "       0.44313725, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.06666667, 0.21176471,\n",
       "       0.62352941, 0.87058824, 0.75686275, 0.81568627, 0.75294118,\n",
       "       0.77254902, 0.78431373, 0.78431373, 0.78431373, 0.78431373,\n",
       "       0.78823529, 0.79607843, 0.76470588, 0.82352941, 0.64705882,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18431373, 0.88235294,\n",
       "       0.75294118, 0.83921569, 0.79607843, 0.80784314, 0.8       ,\n",
       "       0.8       , 0.80392157, 0.80784314, 0.8       , 0.83137255,\n",
       "       0.77254902, 0.85490196, 0.41960784, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00392157, 0.02352941,\n",
       "       0.        , 0.18039216, 0.83137255, 0.76470588, 0.83137255,\n",
       "       0.79215686, 0.80784314, 0.80392157, 0.8       , 0.80392157,\n",
       "       0.80784314, 0.8       , 0.83137255, 0.78431373, 0.85490196,\n",
       "       0.35686275, 0.        , 0.01176471, 0.00392157, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.        , 0.04313725,\n",
       "       0.77254902, 0.78039216, 0.80392157, 0.79215686, 0.80392157,\n",
       "       0.80784314, 0.8       , 0.80392157, 0.81176471, 0.8       ,\n",
       "       0.80392157, 0.80392157, 0.85490196, 0.30196078, 0.        ,\n",
       "       0.01960784, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01176471, 0.        , 0.00784314, 0.74901961, 0.77647059,\n",
       "       0.78823529, 0.80392157, 0.80784314, 0.80392157, 0.80392157,\n",
       "       0.80784314, 0.81960784, 0.80784314, 0.78039216, 0.81960784,\n",
       "       0.85882353, 0.29019608, 0.        , 0.01960784, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00784314, 0.        ,\n",
       "       0.        , 0.7372549 , 0.77254902, 0.78431373, 0.81176471,\n",
       "       0.81176471, 0.8       , 0.81176471, 0.81176471, 0.82352941,\n",
       "       0.81568627, 0.77647059, 0.81176471, 0.86666667, 0.28235294,\n",
       "       0.        , 0.01568627, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00784314, 0.        , 0.        , 0.84313725,\n",
       "       0.77647059, 0.79607843, 0.80784314, 0.81568627, 0.80392157,\n",
       "       0.81176471, 0.81176471, 0.82352941, 0.81568627, 0.78431373,\n",
       "       0.79215686, 0.87058824, 0.29411765, 0.        , 0.01568627,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "       0.        , 0.        , 0.83137255, 0.77647059, 0.81960784,\n",
       "       0.80784314, 0.81960784, 0.80784314, 0.81568627, 0.81176471,\n",
       "       0.82745098, 0.80784314, 0.80392157, 0.77647059, 0.86666667,\n",
       "       0.31372549, 0.        , 0.01176471, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.        , 0.        ,\n",
       "       0.8       , 0.78823529, 0.80392157, 0.81568627, 0.81176471,\n",
       "       0.80392157, 0.82745098, 0.80392157, 0.82352941, 0.82352941,\n",
       "       0.81960784, 0.76470588, 0.86666667, 0.37647059, 0.        ,\n",
       "       0.01176471, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00392157, 0.        , 0.        , 0.79215686, 0.78823529,\n",
       "       0.80392157, 0.81960784, 0.81176471, 0.80392157, 0.83529412,\n",
       "       0.80784314, 0.82352941, 0.81960784, 0.82352941, 0.76078431,\n",
       "       0.85098039, 0.41176471, 0.        , 0.00784314, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00392157, 0.        ,\n",
       "       0.        , 0.8       , 0.8       , 0.80392157, 0.81568627,\n",
       "       0.81176471, 0.80392157, 0.84313725, 0.81176471, 0.82352941,\n",
       "       0.81568627, 0.82745098, 0.75686275, 0.83529412, 0.45098039,\n",
       "       0.        , 0.00784314, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.8       ,\n",
       "       0.81176471, 0.81176471, 0.81568627, 0.80784314, 0.80784314,\n",
       "       0.84313725, 0.82352941, 0.82352941, 0.81176471, 0.83137255,\n",
       "       0.76470588, 0.82352941, 0.4627451 , 0.        , 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "       0.        , 0.        , 0.77647059, 0.81568627, 0.81568627,\n",
       "       0.81568627, 0.8       , 0.81176471, 0.83137255, 0.83137255,\n",
       "       0.82352941, 0.81176471, 0.82745098, 0.76862745, 0.81176471,\n",
       "       0.4745098 , 0.        , 0.00392157, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.        , 0.        ,\n",
       "       0.77647059, 0.82352941, 0.81176471, 0.81568627, 0.80784314,\n",
       "       0.81960784, 0.83529412, 0.83137255, 0.82745098, 0.81176471,\n",
       "       0.82352941, 0.77254902, 0.81176471, 0.48627451, 0.        ,\n",
       "       0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.6745098 , 0.82352941,\n",
       "       0.79607843, 0.78823529, 0.78039216, 0.8       , 0.81176471,\n",
       "       0.80392157, 0.8       , 0.78823529, 0.80392157, 0.77254902,\n",
       "       0.80784314, 0.49803922, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.7372549 , 0.86666667, 0.83921569, 0.91764706,\n",
       "       0.9254902 , 0.93333333, 0.95686275, 0.95686275, 0.95686275,\n",
       "       0.94117647, 0.95294118, 0.83921569, 0.87843137, 0.63529412,\n",
       "       0.        , 0.00784314, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00392157, 0.        , 0.        , 0.54509804,\n",
       "       0.57254902, 0.50980392, 0.52941176, 0.52941176, 0.5372549 ,\n",
       "       0.49019608, 0.48627451, 0.49019608, 0.4745098 , 0.46666667,\n",
       "       0.44705882, 0.50980392, 0.29803922, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9555a6c-a306-43fb-b6a8-14ddb49bb5f5",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "we have taken below values to add the first fully connected hidden layer,\n",
    "\n",
    "#### units=128,  activation='relu',  input_shape=(784,)\n",
    "\n",
    "\n",
    "\n",
    "Let us understand why we have these values,\n",
    "\n",
    "#### 1) units (No of neurons) = 128\n",
    "\n",
    "The artificial neuron receives one or more inputs and sums them to produce an output, then this output is passed throught the activation function\n",
    "\n",
    "Now there are some rules to select the number of neurons but every dataset is different, every problem is different, So a set of rules may not work properly\n",
    "\n",
    "To select an appropriate number of units (neurons), we have to try different number of neurons and select number of neurons that works better for the model performance\n",
    "\n",
    "We have to use systematic experimentation to discover what works best for a specific dataset\n",
    "\n",
    "Here i have selected number on neurons=128 because it works better, and this is on the basis of various experiments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2) activation='relu'\n",
    "\n",
    "The activation function calculates the weighted sum of its input, adds a bias and then decides wheather a neuron should be activated or not\n",
    "\n",
    "In simple word we can say that, 'An activation function decides a signal should be passed forward or not'\n",
    "\n",
    "The activation function adds non linearity to the output of a neuron\n",
    "\n",
    "With the non linear transformation a neural network is capable of learning and performing more complex tasks.\n",
    "\n",
    "We are using 'relu' function in first layer because,\n",
    "\n",
    "It is Non-linear, easily backpropagates the errors and multiple layers of neuron being activated by ReLU function\n",
    "\n",
    "As well as, it is computationally less expensive than tanh and sigmoid functions, RELU learns much faster than sigmoid and Tanh function\n",
    "\n",
    "\n",
    "\n",
    "#### 3) input_shape=(784,)\n",
    "\n",
    "In previous we flattened the dataset, before flattening the dataset shape for x_train is (60000, 28, 28) and shape for x_text is (10000, 28, 28)\n",
    "\n",
    "60000 and 1000 are the number of images (batch size)\n",
    "\n",
    "\n",
    "\n",
    "After flattening the dataset shape for x_train is (60000, 784) and shape for x_text is (10000, 784)\n",
    "\n",
    "60000 and 1000 are the number of images (batch size)\n",
    "\n",
    "\n",
    "\n",
    "The first dimension is the batch size, it's None because it can vary depending on how many examples we give for training\n",
    "\n",
    "hence input shape is (784,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4298087b-2f99-4210-8051-6f41d0a6ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sequential model from the keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e082f4ea-f0b7-47d0-9e77-421d6934c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import layers \n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ae89bca-28f0-4228-81a0-049c678499de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128, activation='relu', input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a988ca-d5a3-467b-a754-af88998d9fb3",
   "metadata": {},
   "source": [
    "The comma after 784 in input_shape=(784,) serves as a notation in Python to create a tuple with a single element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5ced738-44a3-4fed-87a5-26d116f87a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Creating a tuple with a single element: (784,)\n",
    "input_shape_tuple = (784,)\n",
    "print(type(input_shape_tuple))  # Output: <class 'tuple'>\n",
    "\n",
    "# Without the trailing comma, it's considered an integer expression inside parentheses\n",
    "not_a_tuple = (784)\n",
    "print(type(not_a_tuple))  # Output: <class 'int'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8958dd0-b51f-4605-affd-e9f214207b48",
   "metadata": {},
   "source": [
    "In the context of defining the input_shape parameter in Keras (or other libraries like TensorFlow), it expects a tuple to represent the shape of the input data. Therefore, when specifying input_shape=(784,), the trailing comma ensures that it creates a tuple with a single element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cb180a1-d84d-4d99-b65c-d0eef8df9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add dropout layer \n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d599a792-d532-4956-876a-0eb9572b402a",
   "metadata": {},
   "source": [
    "Dropout is indeed a regularization technique used in neural networks to mitigate overfitting. Overfitting occurs when a model learns the training data too well, to the extent that it negatively impacts its ability to generalize to new, unseen data.\n",
    "\n",
    "Dropout helps prevent overfitting by randomly \"dropping out\" (deactivating) a certain percentage of neurons during training\n",
    "\n",
    "During forward propagation (when input passes through the network), the Dropout layer randomly sets a fraction of its neuron outputs to zero. This introduces noise and prevents the network from becoming overly reliant on specific neurons.\n",
    "During backward propagation (during training and parameter updates), only the active neurons (those not dropped out) are considered, and the gradients are back-propagated only through these active neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2755bbaf-79ae-441d-a329-79e8847ff0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cd7963b-1f84-4fee-9929-3566fd80ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d459ec94-bc0e-4765-a4da-639f52fa2183",
   "metadata": {},
   "source": [
    "The softmax activation function is specifically used in neural network models, particularly in the output layer, when dealing with multiclass classification problems. Its primary purpose is to convert raw scores (logits) into probabilities for multiple classes.\n",
    "\n",
    "Purpose of Softmax in Multiclass Classification:\n",
    "\n",
    "Softmax is used to generate a probability distribution across multiple classes.\n",
    "It ensures that the output values represent probabilities that sum up to 1, making it useful for selecting the most likely class prediction among multiple class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2300dfb-ca50-4cb2-92a5-a8d8748260ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f2c6719-2798-4f3d-a4a4-b6ff078e46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model (config the model)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb528fa2-a0e0-4015-9a9d-c864278131b1",
   "metadata": {},
   "source": [
    "optimizer='adam':\n",
    "\n",
    "The optimizer argument specifies the optimization algorithm used during training. In this case, 'adam' refers to the Adam optimizer, which is an adaptive learning rate optimization algorithm widely used in deep learning. It helps adjust the learning rate dynamically for each parameter during training.\n",
    "\n",
    "\n",
    "loss='sparse_categorical_crossentropy':\n",
    "\n",
    "The loss argument specifies the loss function used to measure how well the model performs during training.\n",
    "'sparse_categorical_crossentropy' is a specific type of loss function suitable for multiclass classification problems where the labels are integers (not one-hot encoded)\n",
    "\n",
    "\n",
    "metrics='sparse_categorical_accuracy':\n",
    "\n",
    "The metrics argument specifies the evaluation metric used to monitor the model's performance during training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa9a8f-6cda-44c5-98d7-fc368cb00a05",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89421e69-0384-41cc-9cc1-70074488f8e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:36:14.872493: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 3s 4ms/step - loss: 0.6750 - sparse_categorical_accuracy: 0.7618 - val_loss: 0.4487 - val_sparse_categorical_accuracy: 0.8390\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.4600 - sparse_categorical_accuracy: 0.8351 - val_loss: 0.4119 - val_sparse_categorical_accuracy: 0.8485\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.4200 - sparse_categorical_accuracy: 0.8476 - val_loss: 0.3958 - val_sparse_categorical_accuracy: 0.8559\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3926 - sparse_categorical_accuracy: 0.8587 - val_loss: 0.3824 - val_sparse_categorical_accuracy: 0.8609\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3767 - sparse_categorical_accuracy: 0.8623 - val_loss: 0.3625 - val_sparse_categorical_accuracy: 0.8690\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.3611 - sparse_categorical_accuracy: 0.8680 - val_loss: 0.3780 - val_sparse_categorical_accuracy: 0.8578\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3533 - sparse_categorical_accuracy: 0.8697 - val_loss: 0.3539 - val_sparse_categorical_accuracy: 0.8726\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3431 - sparse_categorical_accuracy: 0.8744 - val_loss: 0.3538 - val_sparse_categorical_accuracy: 0.8749\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3344 - sparse_categorical_accuracy: 0.8778 - val_loss: 0.3447 - val_sparse_categorical_accuracy: 0.8743\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3276 - sparse_categorical_accuracy: 0.8785 - val_loss: 0.3434 - val_sparse_categorical_accuracy: 0.8748\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3268 - sparse_categorical_accuracy: 0.8799 - val_loss: 0.3441 - val_sparse_categorical_accuracy: 0.8735\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.3195 - sparse_categorical_accuracy: 0.8812 - val_loss: 0.3335 - val_sparse_categorical_accuracy: 0.8813\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3078 - sparse_categorical_accuracy: 0.8868 - val_loss: 0.3344 - val_sparse_categorical_accuracy: 0.8811\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3045 - sparse_categorical_accuracy: 0.8872 - val_loss: 0.3351 - val_sparse_categorical_accuracy: 0.8766\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3019 - sparse_categorical_accuracy: 0.8881 - val_loss: 0.3377 - val_sparse_categorical_accuracy: 0.8747\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2974 - sparse_categorical_accuracy: 0.8892 - val_loss: 0.3297 - val_sparse_categorical_accuracy: 0.8798\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2906 - sparse_categorical_accuracy: 0.8919 - val_loss: 0.3231 - val_sparse_categorical_accuracy: 0.8851\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2877 - sparse_categorical_accuracy: 0.8931 - val_loss: 0.3286 - val_sparse_categorical_accuracy: 0.8806\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 0.2846 - sparse_categorical_accuracy: 0.8937 - val_loss: 0.3235 - val_sparse_categorical_accuracy: 0.8868\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.2830 - sparse_categorical_accuracy: 0.8938 - val_loss: 0.3334 - val_sparse_categorical_accuracy: 0.8787\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.2818 - sparse_categorical_accuracy: 0.8943 - val_loss: 0.3240 - val_sparse_categorical_accuracy: 0.8832\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2787 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.3234 - val_sparse_categorical_accuracy: 0.8845\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2753 - sparse_categorical_accuracy: 0.8964 - val_loss: 0.3308 - val_sparse_categorical_accuracy: 0.8811\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2727 - sparse_categorical_accuracy: 0.8974 - val_loss: 0.3166 - val_sparse_categorical_accuracy: 0.8892\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2712 - sparse_categorical_accuracy: 0.8978 - val_loss: 0.3146 - val_sparse_categorical_accuracy: 0.8892\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.2661 - sparse_categorical_accuracy: 0.8999 - val_loss: 0.3164 - val_sparse_categorical_accuracy: 0.8893\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2645 - sparse_categorical_accuracy: 0.9005 - val_loss: 0.3179 - val_sparse_categorical_accuracy: 0.8899\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2616 - sparse_categorical_accuracy: 0.9009 - val_loss: 0.3301 - val_sparse_categorical_accuracy: 0.8861\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2635 - sparse_categorical_accuracy: 0.9007 - val_loss: 0.3162 - val_sparse_categorical_accuracy: 0.8900\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2579 - sparse_categorical_accuracy: 0.9033 - val_loss: 0.3152 - val_sparse_categorical_accuracy: 0.8890\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2598 - sparse_categorical_accuracy: 0.9036 - val_loss: 0.3205 - val_sparse_categorical_accuracy: 0.8898\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2557 - sparse_categorical_accuracy: 0.9031 - val_loss: 0.3252 - val_sparse_categorical_accuracy: 0.8836\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2529 - sparse_categorical_accuracy: 0.9043 - val_loss: 0.3302 - val_sparse_categorical_accuracy: 0.8875\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2512 - sparse_categorical_accuracy: 0.9047 - val_loss: 0.3244 - val_sparse_categorical_accuracy: 0.8896\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2500 - sparse_categorical_accuracy: 0.9049 - val_loss: 0.3231 - val_sparse_categorical_accuracy: 0.8891\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2475 - sparse_categorical_accuracy: 0.9065 - val_loss: 0.3232 - val_sparse_categorical_accuracy: 0.8884\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2489 - sparse_categorical_accuracy: 0.9069 - val_loss: 0.3218 - val_sparse_categorical_accuracy: 0.8902\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2470 - sparse_categorical_accuracy: 0.9074 - val_loss: 0.3147 - val_sparse_categorical_accuracy: 0.8904\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2449 - sparse_categorical_accuracy: 0.9073 - val_loss: 0.3245 - val_sparse_categorical_accuracy: 0.8857\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2431 - sparse_categorical_accuracy: 0.9071 - val_loss: 0.3275 - val_sparse_categorical_accuracy: 0.8903\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2408 - sparse_categorical_accuracy: 0.9096 - val_loss: 0.3279 - val_sparse_categorical_accuracy: 0.8857\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2387 - sparse_categorical_accuracy: 0.9095 - val_loss: 0.3224 - val_sparse_categorical_accuracy: 0.8906\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2385 - sparse_categorical_accuracy: 0.9100 - val_loss: 0.3214 - val_sparse_categorical_accuracy: 0.8931\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2392 - sparse_categorical_accuracy: 0.9113 - val_loss: 0.3211 - val_sparse_categorical_accuracy: 0.8937\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 0.2373 - sparse_categorical_accuracy: 0.9116 - val_loss: 0.3307 - val_sparse_categorical_accuracy: 0.8911\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2340 - sparse_categorical_accuracy: 0.9110 - val_loss: 0.3246 - val_sparse_categorical_accuracy: 0.8919\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2353 - sparse_categorical_accuracy: 0.9122 - val_loss: 0.3222 - val_sparse_categorical_accuracy: 0.8912\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.2305 - sparse_categorical_accuracy: 0.9125 - val_loss: 0.3273 - val_sparse_categorical_accuracy: 0.8894\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2333 - sparse_categorical_accuracy: 0.9124 - val_loss: 0.3164 - val_sparse_categorical_accuracy: 0.8918\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2305 - sparse_categorical_accuracy: 0.9131 - val_loss: 0.3266 - val_sparse_categorical_accuracy: 0.8891\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2302 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.3320 - val_sparse_categorical_accuracy: 0.8873\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2297 - sparse_categorical_accuracy: 0.9126 - val_loss: 0.3183 - val_sparse_categorical_accuracy: 0.8933\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2267 - sparse_categorical_accuracy: 0.9142 - val_loss: 0.3256 - val_sparse_categorical_accuracy: 0.8923\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2283 - sparse_categorical_accuracy: 0.9131 - val_loss: 0.3240 - val_sparse_categorical_accuracy: 0.8909\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2273 - sparse_categorical_accuracy: 0.9145 - val_loss: 0.3215 - val_sparse_categorical_accuracy: 0.8908\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2213 - sparse_categorical_accuracy: 0.9163 - val_loss: 0.3242 - val_sparse_categorical_accuracy: 0.8943\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2206 - sparse_categorical_accuracy: 0.9166 - val_loss: 0.3270 - val_sparse_categorical_accuracy: 0.8917\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2207 - sparse_categorical_accuracy: 0.9178 - val_loss: 0.3270 - val_sparse_categorical_accuracy: 0.8926\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2199 - sparse_categorical_accuracy: 0.9168 - val_loss: 0.3152 - val_sparse_categorical_accuracy: 0.8948\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2209 - sparse_categorical_accuracy: 0.9175 - val_loss: 0.3231 - val_sparse_categorical_accuracy: 0.8908\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2185 - sparse_categorical_accuracy: 0.9173 - val_loss: 0.3270 - val_sparse_categorical_accuracy: 0.8948\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2178 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.3205 - val_sparse_categorical_accuracy: 0.8953\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2150 - sparse_categorical_accuracy: 0.9196 - val_loss: 0.3401 - val_sparse_categorical_accuracy: 0.8918\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2178 - sparse_categorical_accuracy: 0.9174 - val_loss: 0.3216 - val_sparse_categorical_accuracy: 0.8942\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2177 - sparse_categorical_accuracy: 0.9171 - val_loss: 0.3319 - val_sparse_categorical_accuracy: 0.8917\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2124 - sparse_categorical_accuracy: 0.9191 - val_loss: 0.3327 - val_sparse_categorical_accuracy: 0.8929\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2134 - sparse_categorical_accuracy: 0.9187 - val_loss: 0.3239 - val_sparse_categorical_accuracy: 0.8937\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.2132 - sparse_categorical_accuracy: 0.9192 - val_loss: 0.3229 - val_sparse_categorical_accuracy: 0.8922\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.2091 - sparse_categorical_accuracy: 0.9215 - val_loss: 0.3256 - val_sparse_categorical_accuracy: 0.8933\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.2109 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.3318 - val_sparse_categorical_accuracy: 0.8937\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.2103 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.3400 - val_sparse_categorical_accuracy: 0.8914\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.2077 - sparse_categorical_accuracy: 0.9219 - val_loss: 0.3342 - val_sparse_categorical_accuracy: 0.8931\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.2082 - sparse_categorical_accuracy: 0.9215 - val_loss: 0.3296 - val_sparse_categorical_accuracy: 0.8939\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2066 - sparse_categorical_accuracy: 0.9220 - val_loss: 0.3323 - val_sparse_categorical_accuracy: 0.8947\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2048 - sparse_categorical_accuracy: 0.9225 - val_loss: 0.3350 - val_sparse_categorical_accuracy: 0.8940\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2055 - sparse_categorical_accuracy: 0.9222 - val_loss: 0.3356 - val_sparse_categorical_accuracy: 0.8901\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2050 - sparse_categorical_accuracy: 0.9228 - val_loss: 0.3397 - val_sparse_categorical_accuracy: 0.8933\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.2014 - sparse_categorical_accuracy: 0.9242 - val_loss: 0.3405 - val_sparse_categorical_accuracy: 0.8936\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2074 - sparse_categorical_accuracy: 0.9212 - val_loss: 0.3628 - val_sparse_categorical_accuracy: 0.8893\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2069 - sparse_categorical_accuracy: 0.9216 - val_loss: 0.3401 - val_sparse_categorical_accuracy: 0.8933\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2018 - sparse_categorical_accuracy: 0.9233 - val_loss: 0.3371 - val_sparse_categorical_accuracy: 0.8932\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2018 - sparse_categorical_accuracy: 0.9230 - val_loss: 0.3384 - val_sparse_categorical_accuracy: 0.8933\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2007 - sparse_categorical_accuracy: 0.9244 - val_loss: 0.3410 - val_sparse_categorical_accuracy: 0.8905\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.1986 - sparse_categorical_accuracy: 0.9243 - val_loss: 0.3418 - val_sparse_categorical_accuracy: 0.8927\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2003 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.3323 - val_sparse_categorical_accuracy: 0.8896\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.1979 - sparse_categorical_accuracy: 0.9244 - val_loss: 0.3458 - val_sparse_categorical_accuracy: 0.8933\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.1990 - sparse_categorical_accuracy: 0.9234 - val_loss: 0.3436 - val_sparse_categorical_accuracy: 0.8957\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.1985 - sparse_categorical_accuracy: 0.9247 - val_loss: 0.3386 - val_sparse_categorical_accuracy: 0.8911\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.1945 - sparse_categorical_accuracy: 0.9250 - val_loss: 0.3507 - val_sparse_categorical_accuracy: 0.8914\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.1997 - sparse_categorical_accuracy: 0.9253 - val_loss: 0.3404 - val_sparse_categorical_accuracy: 0.8911\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.1958 - sparse_categorical_accuracy: 0.9255 - val_loss: 0.3481 - val_sparse_categorical_accuracy: 0.8912\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.1964 - sparse_categorical_accuracy: 0.9259 - val_loss: 0.3393 - val_sparse_categorical_accuracy: 0.8916\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 0.1948 - sparse_categorical_accuracy: 0.9256 - val_loss: 0.3495 - val_sparse_categorical_accuracy: 0.8906\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 0.1915 - sparse_categorical_accuracy: 0.9268 - val_loss: 0.3454 - val_sparse_categorical_accuracy: 0.8941\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.1958 - sparse_categorical_accuracy: 0.9251 - val_loss: 0.3560 - val_sparse_categorical_accuracy: 0.8937\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.1918 - sparse_categorical_accuracy: 0.9273 - val_loss: 0.3491 - val_sparse_categorical_accuracy: 0.8914\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.1926 - sparse_categorical_accuracy: 0.9279 - val_loss: 0.3516 - val_sparse_categorical_accuracy: 0.8899\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.1917 - sparse_categorical_accuracy: 0.9273 - val_loss: 0.3651 - val_sparse_categorical_accuracy: 0.8919\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.1914 - sparse_categorical_accuracy: 0.9283 - val_loss: 0.3487 - val_sparse_categorical_accuracy: 0.8917\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.1886 - sparse_categorical_accuracy: 0.9280 - val_loss: 0.3490 - val_sparse_categorical_accuracy: 0.8932\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=100, epochs=100, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac8b5847-6c1f-49f5-b8de-afdf0f80b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3490 - sparse_categorical_accuracy: 0.8932\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab9fd124-6d09-40a0-85ea-0d09e82cf5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tess Loss: 0.349006712436676\n",
      "Tess Accuracy: 0.8931999802589417\n"
     ]
    }
   ],
   "source": [
    "print(\"Tess Loss: {}\".format(evaluation[0]))\n",
    "print(\"Tess Accuracy: {}\".format(evaluation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2e382a6-c569-4cc8-9054-fcfd30542e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 723us/step\n"
     ]
    }
   ],
   "source": [
    "#model preditction \n",
    "\n",
    "predict_value = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ef5abb3-acae-4206-bd25-a4f09afa3bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.27308191e-08, 1.05665650e-12, 1.49810567e-10, ...,\n",
       "        1.35917589e-05, 6.07588199e-13, 9.99981999e-01],\n",
       "       [3.71988556e-08, 2.26284256e-17, 9.98634398e-01, ...,\n",
       "        3.59768274e-26, 1.06183190e-17, 1.06511714e-26],\n",
       "       [2.72975240e-16, 1.00000000e+00, 5.42309667e-21, ...,\n",
       "        1.40405305e-30, 2.83022845e-19, 1.29730460e-29],\n",
       "       ...,\n",
       "       [6.56172328e-13, 9.54090939e-22, 1.14718373e-15, ...,\n",
       "        3.67991717e-16, 1.00000000e+00, 8.77964575e-22],\n",
       "       [1.35120119e-13, 1.00000000e+00, 5.66833580e-19, ...,\n",
       "        1.65372388e-21, 1.46224016e-13, 1.54632320e-20],\n",
       "       [2.91680222e-11, 2.97888501e-18, 5.85812301e-12, ...,\n",
       "        7.08129446e-05, 3.44695772e-10, 1.45864796e-11]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a4e45-7ec5-4f28-9004-e1627abaf44a",
   "metadata": {},
   "source": [
    "you can use the predict() method to get the predicted class probabilities and then obtain the class labels using numpy.argmax()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "206da6cd-994c-4184-93c3-3df1181fb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the predicted class labels (indices with maximum probability for each sample)\n",
    "predict_classes = np.argmax(predict_value, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b06f253b-b212-4d5e-b88b-9490e1652960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, ..., 8, 1, 5])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afc101-0b38-4047-952f-8431bafe77a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_PROJECTS_Kernel",
   "language": "python",
   "name": "ml_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
